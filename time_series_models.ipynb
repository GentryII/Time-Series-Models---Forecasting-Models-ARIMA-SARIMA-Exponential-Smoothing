{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f4162df",
   "metadata": {},
   "source": [
    "**Introduction** \n",
    "The following notebook contains code which was used to develop time series machine learning models for the forecasting of Mobile Network Operators traffic in Malawi. The models were trained on annomyzed call detail (CDR) data. Three machine learning models were developed which were: ARIMA, SARIMA and Exponential Smoothing. The ARIMA model emerged as the most accurate model based on Mean Absolute Error (MAE), Root Mean Absolute Error (RMSE) and Mean Squared Error (MSE). The models were developed using the Python programming language on a 2i2c cloud platform for research. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f5ad79",
   "metadata": {},
   "source": [
    "**Data Preprocessing** - Import the necessary libraries for data preparation for model training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02816f27",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "083246bd",
   "metadata": {},
   "source": [
    "Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f788d92",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('data_copy.csv')\n",
    "\n",
    "# Extract year\n",
    "data['year'] = pd.to_datetime(data['period_id'], format='%Y%m%d').dt.year\n",
    "\n",
    "# Extract month\n",
    "data['month'] = pd.to_datetime(data['period_id'], format='%Y%m%d').dt.month_name()  # Month name (e.g., January)\n",
    "\n",
    "# Extract day of month\n",
    "data['day_of_month'] = pd.to_datetime(data['period_id'], format='%Y%m%d').dt.day\n",
    "\n",
    "# Extract day of week \n",
    "data['day_of_week'] = pd.to_datetime(data['period_id'], format='%Y%m%d').dt.strftime('%A')  # Day of week (e.g., Monday)\n",
    "\n",
    "# You can add similar logic to extract other features like quarter or week of year (if needed)\n",
    "data = data.sort_values(by='period_id')\n",
    "# Select rows where 'teleservice_cat' is 'VOICE'\n",
    "voice_data = data[data['teleservice_cat'] == 'VOICE']\n",
    "voice_data_not_transformed = voice_data.ffill()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af7cec5",
   "metadata": {},
   "source": [
    "**Outlier Detection** - z-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ef58b1",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Calculate z-scores\n",
    "z_scores = (voice_data - voice_data.mean()) / voice_data.std()\n",
    "# Identify outliers based on z-score threshold (e.g., +/- 3)\n",
    "outliers = voice_data[abs(z_scores) > 3]\n",
    "\n",
    "# Replace outliers with the mean\n",
    "handled_outliers = voice_data.where(~(abs(z_scores) > 3), voice_data.mean())\n",
    "\n",
    "model_data = np.log1p(voice_data+ 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d9f5769",
   "metadata": {},
   "source": [
    "**One Hot Encoding** Preparing the data further for model training and testing. This started with importing the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20839af0",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from datetime import datetime\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a3cdeeb",
   "metadata": {},
   "source": [
    "One Hot Encoding the the selected features for model training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5caee0fe",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('data_copy.csv')\n",
    "\n",
    "# Extract year, month, day of month, and day of week\n",
    "data['year'] = pd.to_datetime(data['period_id'], format='%Y%m%d').dt.year\n",
    "data['month'] = pd.to_datetime(data['period_id'], format='%Y%m%d').dt.month_name()\n",
    "data['day_of_month'] = pd.to_datetime(data['period_id'], format='%Y%m%d').dt.day\n",
    "data['day_of_week'] = pd.to_datetime(data['period_id'], format='%Y%m%d').dt.strftime('%A')\n",
    "\n",
    "data = data.sort_values(by='period_id')\n",
    "\n",
    "# Select rows where 'teleservice_cat' is 'VOICE'\n",
    "voice_data = data[data['teleservice_cat'] == 'VOICE']\n",
    "\n",
    "\n",
    "categorical_columns = [\"day_of_week\"]\n",
    "\n",
    "# Create a OneHotEncoder object\n",
    "encoder = OneHotEncoder(handle_unknown='ignore')  # 'ignore' handles unseen categories\n",
    "\n",
    "# Fit the encoder on the categorical data (learns the categories)\n",
    "encoder.fit(voice_data[categorical_columns])\n",
    "\n",
    "# Transform the data using the fitted encoder (creates one-hot encoded columns)\n",
    "encoded_data = pd.DataFrame(encoder.transform(voice_data[categorical_columns]))\n",
    "\n",
    "\n",
    "# Combine the encoded data with the rest of the data\n",
    "voice_data = pd.concat([voice_data, encoded_data], axis=1)\n",
    "\n",
    "voice_data = voice_data.dropna()\n",
    "\n",
    "\n",
    "voice_data['day_of_week'] = voice_data['day_of_week'].astype('category')\n",
    "\n",
    "categorical_columns = [\"day_of_week\"]\n",
    "encoder = OneHotEncoder(handle_unknown='ignore')  # 'ignore' handles unseen categories\n",
    "\n",
    "# Fit the encoder on the categorical data (learns the categories)\n",
    "encoder.fit(voice_data[categorical_columns])\n",
    "\n",
    "# Transform the data using the fitted encoder (creates one-hot encoded columns)\n",
    "encoded_data = pd.DataFrame(encoder.transform(voice_data[categorical_columns]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "182c9c3b",
   "metadata": {},
   "source": [
    "**Statistical Tests and Analysis** Time Series Plot and Dickey-Fuller Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51756f17",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Create time series plot\n",
    "plt.figure(figsize=(7, 5))\n",
    "plt.plot(time_steps, filled_data, marker='o', linestyle='-')\n",
    "plt.xlabel('Time Steps')\n",
    "plt.ylabel('Data Values')\n",
    "plt.title('Time Series Plot')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Perform Dickey-Fuller test\n",
    "adf_result = adfuller(filled_data)\n",
    "\n",
    "# Print test statistics\n",
    "print('ADF Statistic:', adf_result[0])\n",
    "print('p-value:', adf_result[1])\n",
    "print('Critical Values:')\n",
    "for key, value in adf_result[4].items():\n",
    "  print('\\t{}: {}'.format(key, value))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99b2ec8b",
   "metadata": {},
   "source": [
    "**Statistical Tests and Analysis** Partial Autocorrelation Function (PACF) and Autocorrelation Function (ACF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c0d72e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Calculate ACF and PACF with confidence intervals\n",
    "lags = range(20)\n",
    "confint_alpha = 0.05  # Confidence level (e.g., 95% confidence interval)\n",
    "\n",
    "acf_values, confint_acf = acf(filled_data, nlags=19, alpha=confint_alpha)\n",
    "pacf_values, confint_pacf = pacf(filled_data, nlags=19, alpha=confint_alpha)\n",
    "\n",
    "# Plot ACF on ax1\n",
    "ax1.plot(lags, acf_values)\n",
    "ax1.fill_between(lags, confint_acf[:, 0], confint_acf[:, 1], color='black', alpha=0.9, label='Confidence Interval-ACF')\n",
    "ax1.set_xlabel('Lags')\n",
    "ax1.set_ylabel('Autocorrelation')\n",
    "ax1.set_title('Autocorrelation Function (ACF)')\n",
    "ax1.axhline(y=0, color='r', linestyle='--', label='Significance Level-ACF')  # Optional significance line\n",
    "\n",
    "# Plot PACF on ax2\n",
    "ax2.plot(lags, pacf_values)\n",
    "ax2.fill_between(lags, confint_pacf[:, 0], confint_pacf[:, 1], color='orange', alpha=0.9, label='Confidence Interval-PACF')\n",
    "ax2.set_xlabel('Lags')\n",
    "ax2.set_ylabel('Partial Autocorrelation')\n",
    "ax2.set_title('Partial Autocorrelation Function (PACF)')\n",
    "ax2.axhline(y=0, color='black', linestyle='--', label='Significance Level-PACF')  # Optional significance line\n",
    "\n",
    "# Add legends and display the plot\n",
    "lines1, labels1 = ax1.get_legend_handles_labels()\n",
    "lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "plt.legend(lines1 + lines2, labels1 + labels2, loc='upper right')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "006fe9e0",
   "metadata": {},
   "source": [
    "**Exponential Smoothing** We start with code for the exponential smoothing algorithm begining with importing the necessary machine learning python libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "062fde74",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3086c90c",
   "metadata": {},
   "source": [
    "**Function Definition for the model:** We defined all the necessary functions for the training and evaluation of the the model "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c5580a",
   "metadata": {},
   "source": [
    "# Define parameter grid for alpha\n",
    "param_grid = {'alpha': np.linspace(0.05, 0.95, 19)}  \n",
    "\n",
    "# Define scoring metric\n",
    "def mse(y_true, y_pred):\n",
    "    return mean_squared_error(y_true, y_pred)\n",
    "\n",
    "\n",
    "def mae(y_true, y_pred):  # Define mae function \n",
    "  return mean_absolute_error(y_true, y_pred)\n",
    "\n",
    "def rmse(y_true, y_pred):\n",
    "  return np.sqrt(mean_squared_error(y_true, y_pred)) \n",
    "\n",
    "best_model = None\n",
    "best_score = np.inf\n",
    "# Define time series split for cross-validation\n",
    "tscv = TimeSeriesSplit(n_splits=5)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0589ead8",
   "metadata": {},
   "source": [
    "**Grid Search** Perform the Grid Search to find the optimal hyperparameter combination for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46cf3bc4",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Grid search loop\n",
    "for params in param_grid.values():\n",
    "    # Perform cross-validation\n",
    "                cv_scores = []\n",
    "                cv_scores2 = []\n",
    "                cv_scores3 = []\n",
    "                for train_index, test_index in tscv.split(filled_data):\n",
    "                    train, validation = filled_data.iloc[train_index], filled_data.iloc[test_index]\n",
    "                    model = ExponentialSmoothing(filled_data, trend=None, seasonal=None, seasonal_periods=None)\n",
    "                    alpha_value = params  # Extract the current alpha value from params\n",
    "                    model_fit = model.fit(smoothing_level=0.05)\n",
    "                    predictions = model_fit.forecast(steps=len(validation))\n",
    "                    rmse = np.sqrt(mean_squared_error(validation, predictions))\n",
    "                    mae = mean_absolute_error(validation, predictions)\n",
    "                    mse = mean_squared_error(validation, predictions)\n",
    "                    absolute_errors = np.abs(filled_data - predictions)\n",
    "                    absolute_errors_numeric = pd.to_numeric(absolute_errors, errors='coerce')\n",
    "                   \n",
    "\n",
    "                    print(\"RMSE\")\n",
    "                    print(rmse)\n",
    "                    print(\"MAE\")\n",
    "                    print(mae)\n",
    "                    print(\"MSE\")\n",
    "                    print(mse)\n",
    "                    #print(mape)\n",
    "\n",
    "                    cv_scores.append(rmse)\n",
    "                    cv_scores2.append(mae)\n",
    "                    cv_scores3.append(mse)\n",
    "\n",
    "                    print(f\"Final RMSE: {min(cv_scores)}\")\n",
    "                    print(f\"Final MAE: {min(cv_scores2)}\")\n",
    "                    print(f\"Final MSE: {min(cv_scores3)}\")\n",
    "                    \n",
    "                mean_cv_score = np.mean(cv_scores)\n",
    "                #if mean_cv_score < best_score:\n",
    "                best_score, best_model = mean_cv_score, model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3111864",
   "metadata": {},
   "source": [
    "Code for the visuals for the final results from the Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8312356",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "plt.scatter(validation, predictions)\n",
    "plt.plot(validation[-20:], validation[-20:], color=\"red\", linestyle=\"--\")\n",
    "plt.xlabel(\"Actual Values\")\n",
    "plt.ylabel(\"Predicted Values\")\n",
    "plt.title(\"Predicted vs. Actual Values\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e1f2e86",
   "metadata": {},
   "source": [
    "**SARIMA** The following is the code for the Seasonal Autoregressive Moving Average Model, starting with the import of the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e4a431",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "from datetime import datetime\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from scipy import sparse\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.tsa.stattools import acf, pacf\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e5d83b0",
   "metadata": {},
   "source": [
    "**SARIMA Model**: The data is split into train-test splits and the hyperparameters are initialized based on statistical results from data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69128e1d",
   "metadata": {},
   "source": [
    "# Define hyperparameter ranges\n",
    "p_values = [3]  # Adjust as needed\n",
    "d_values = [0, 1]  # Adjust as needed\n",
    "q_values = [0, 1, 2]  # Adjust as needed\n",
    "\n",
    "# Create time series cross-validation object\n",
    "tscv = TimeSeriesSplit(n_splits=5)  # Adjust n_splits as needed\n",
    "\n",
    "# Initialize variables for best model tracking\n",
    "best_score = float('inf')\n",
    "best_model = None\n",
    "best_order = None\n",
    "\n",
    "# Perform hyperparameter tuning and cross-validation with exogenous variables\n",
    "for p in p_values:\n",
    "    for d in d_values:\n",
    "        for q in q_values:\n",
    "            order = (p, d, q)\n",
    "            seasonal_order = (0, 0, 1, 7)  # No seasonality assumed here\n",
    "            try:\n",
    "                cv_scores = []\n",
    "                cv_scores2 = []\n",
    "                cv_scores3 = []\n",
    "\n",
    "                for train_index, test_index in tscv.split(voice_data_copy):\n",
    "                    train, validation = voice_data_copy.iloc[train_index], voice_data_copy.iloc[test_index]\n",
    "\n",
    "\n",
    "                \n",
    "                    # Ensure numerical dtypes\n",
    "                    #train['total_duration'] = train['total_duration'].astype('float64')  # Adjust dtype as needed\n",
    "                    \n",
    "                    # Correct exog input\n",
    "                    exog_train = train[['day_of_week_Friday', 'day_of_week_Monday', 'day_of_week_Saturday', \n",
    "                                        'day_of_week_Sunday', 'day_of_week_Thursday', 'day_of_week_Tuesday', \n",
    "                                        'day_of_week_Wednesday', 'month_April', 'month_August','month_December', \n",
    "                                        'month_February', 'month_January', 'month_July', 'month_June', 'month_March', \n",
    "                                        'month_May', 'month_November', 'month_October', 'month_September']]\n",
    "                    exog_validate = validation[['day_of_week_Friday', 'day_of_week_Monday', 'day_of_week_Saturday', \n",
    "                                        'day_of_week_Sunday', 'day_of_week_Thursday', 'day_of_week_Tuesday', \n",
    "                                        'day_of_week_Wednesday', 'month_April', 'month_August','month_December', \n",
    "                                        'month_February', 'month_January', 'month_July', 'month_June', 'month_March', \n",
    "                                        'month_May', 'month_November', 'month_October', 'month_September']]\n",
    "                    \n",
    "                    # Select columns for training/validation\n",
    "                    #training = np.asarray(train[['total_volume']])  # Convert to NumPy array\n",
    "                    training = train[['volume_traffic']].to_numpy()  # Select and convert to NumPy array\n",
    "                    validation_data = validation[['volume_traffic']].to_numpy()\n",
    "                    training2 = train['volume_traffic']\n",
    "\n",
    "                    training_date_data = train.copy()\n",
    "\n",
    "                    training_date_data['period_id'] = pd.to_datetime(training_date_data['period_id'].astype(str)[:8], format='mixed', errors='coerce')\n",
    "\n",
    "\n",
    "\n",
    "                    #numeric_training = np.asarray(training, dtype=float)  # Convert to float64 NumPy array\n",
    "                    #feature_index = 2 \n",
    "                    #univariate_series = training[:, feature_index]\n",
    "                    #training = np.asarray(train[['total_volume']])\n",
    "                    training_exog = np.asarray(exog_train)\n",
    "                    validation_exog = np.asarray(exog_validate)\n",
    "                    #validation_y = np.asarray(validation[['volume_traffic']])\n",
    "                    #forecast_horizon = len(exog_validate)\n",
    "                    \n",
    "                    # Include exogenous variables\n",
    "                    #model_with_exog = ARIMA(x_train, order=order, exog=exog_train)\n",
    "                    #model_fit_exog = model_with_exog.fit()\n",
    "\n",
    "\n",
    "                    # Include exogenous variables in ARIMA model\n",
    "                    #model_with_exog = ARIMA(first_half, order=order, exog=training_exog)\n",
    "                    # Include exogenous variables\n",
    "                    model_with_exog = SARIMAX(training, order=order, exog=exog_train,\n",
    "                                  seasonal_order=seasonal_order)\n",
    "                    model_fit_exog = model_with_exog.fit()\n",
    "\n",
    "                    stepping = len(validation)\n",
    "\n",
    "                    # Generate predictions\n",
    "                    predictions = model_fit_exog.forecast(steps=stepping, exog=validation_exog, alpha=0.05)\n",
    "                    #conf = model_fit_exog.forecast(steps=stepping, exog=validation_exog, alpha=0.05) # 95% conf\n",
    "                    \n",
    "                    \n",
    "\n",
    "                    validation_data = validation_data.reshape(-1) \n",
    "\n",
    "                    errors = validation_data-predictions\n",
    "                    data_length = len(validation)\n",
    "                    time_series = pd.date_range(start='2019-01-01', periods=data_length, freq='D')  # Monthly data for a year\n",
    "                    \n",
    "                    #actual_values = pd.Series([10, 12, 15, 18, 21, 23, 22, 20, 18, 16, 14, 12])\n",
    "                    #predictions = pd.Series([11, 13, 14, 17, 20, 22, 21, 19, 17, 15, 13, 11])\n",
    "\n",
    "\n",
    "                    mae = np.mean(np.abs(errors))  # Mean Absolute Error\n",
    "                    mse = np.mean(errors**2)  # Mean Squared Error\n",
    "                    rmse = np.sqrt(mse)  # Root Mean Squared Error\n",
    "                    prediction_series = pd.Series(predictions)\n",
    "                    validation_series = pd.Series(validation_data)\n",
    "\n",
    "                    training_one_dim = training.reshape(-1) \n",
    "                    training_series = pd.Series(training_one_dim)\n",
    "                    print(type(prediction_series))\n",
    "                    print(type(validation_series))\n",
    "                    print(f\"MAE: {mae:.4f}\")\n",
    "                    print(f\"MSE: {mse:.4f}\")\n",
    "                    print(f\"RMSE: {rmse:.4f}\")\n",
    "\n",
    "                    # Get residuals\n",
    "                    residuals = model_fit_exog.resid\n",
    "                    \n",
    "                    # Get predicted values\n",
    "                    predicted_values = model_fit_exog.predict(dynamic=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d63fcb94",
   "metadata": {},
   "source": [
    "Code for the visuals for the final model performance results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edabbcf4",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "  display(training_date_data)\n",
    "                    # Plot residuals vs. predicted values\n",
    "                    plt.scatter(predicted_values, residuals)\n",
    "                    plt.xlabel(\"Predicted Values\")\n",
    "                    plt.ylabel(\"Residuals\")\n",
    "                    plt.title(\"Residuals vs. Predicted Values\")\n",
    "                    plt.show()\n",
    "                    # Plot ACF of residuals\n",
    "                    fig, ax1 = plt.subplots()\n",
    "                    acf_values = acf(residuals, nlags=40)  # \n",
    "                    plt.plot(acf_values, marker='o', linestyle='-', alpha=0.7)\n",
    "                    plt.axhline(y=1.96/np.sqrt(len(residuals)), color='r', linestyle='--', label='Upper Bound')\n",
    "                    plt.axhline(y=-1.96/np.sqrt(len(residuals)), color='r', linestyle='--', label='Lower Bound')\n",
    "                    plt.xlabel(\"Lags\")\n",
    "                    plt.ylabel(\"Autocorrelation\")\n",
    "                    plt.title(\"Autocorrelation Function (ACF) of Residuals\")\n",
    "                    plt.legend()\n",
    "                    plt.tight_layout()\n",
    "                    plt.show()\n",
    "\n",
    "                    # Plot PACF of residuals\n",
    "                    fig, ax2 = plt.subplots()\n",
    "                    pacf_values = pacf(residuals, nlags=40)  # \n",
    "                    plt.plot(pacf_values, marker='o', linestyle='-', alpha=0.7)\n",
    "                    plt.axhline(y=1.96/np.sqrt(len(residuals)), color='r', linestyle='--', label='Upper Bound')\n",
    "                    plt.axhline(y=-1.96/np.sqrt(len(residuals)), color='r', linestyle='--', label='Lower Bound')\n",
    "                    plt.xlabel(\"Lags\")\n",
    "                    plt.ylabel(\"Partial Autocorrelation\")\n",
    "                    plt.title(\"Partial Autocorrelation Function (PACF) of Residuals\")\n",
    "                    plt.legend()\n",
    "                    plt.tight_layout()\n",
    "                    plt.show()\n",
    "\n",
    "                    time_index = pd.date_range(start='2019-04-01', end='2019-04-07', freq='D')  \n",
    "                    prediction_plot = prediction_series.head(7)\n",
    "                    validation_plot = validation_series.head(7)\n",
    "                    training_plot = training_series.head(7)\n",
    "                    prediction_plot.index = time_index\n",
    "                    validation_plot.index = time_index\n",
    "            \n",
    "\n",
    "                    filtered_predictions = prediction_plot['2019-04-01':'2019-04-30']\n",
    "                    filtered_validation = validation_plot['2019-04-01':'2019-04-30']\n",
    "\n",
    "                    plt.plot(filtered_predictions, label='Predictions')\n",
    "                    plt.plot(filtered_validation, label='Validation')\n",
    "                    plt.title(\"Predictions for a 7 Day Period\")\n",
    "                    plt.tight_layout()\n",
    "                    plt.legend()\n",
    "                    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4fc8bf3",
   "metadata": {},
   "source": [
    "**ARIMA** The following is the code for the Autoregressive Moving Average Model, starting with the import of the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad2a390a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import timedelta\n",
    "import pandas as pd\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from IPython.display import display\n",
    "from statsmodels.graphics.tsaplots import plot_acf\n",
    "from statsmodels.tsa.stattools import kpss\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.tsa.stattools import acf, pacf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "360b3848",
   "metadata": {},
   "source": [
    "**ARIMA Model**: The data is split into train-test splits and the hyperparameters are initialized based on statistical results from data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57cc7501",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "p = 1\n",
    "d = 0\n",
    "q = 0\n",
    "\n",
    "# Define time series split for cross-validation\n",
    "tscv = TimeSeriesSplit(n_splits=5)  \n",
    "\n",
    "# Define hyperparameter grid\n",
    "p_values = [1, 2, 3]\n",
    "d_values = [0, 1]\n",
    "q_values = [1, 2]\n",
    "\n",
    "p = float(\"inf\")\n",
    "\n",
    "# Perform grid search\n",
    "best_score, best_model = float(\"inf\"), None\n",
    "best_order = None\n",
    "\n",
    "for p in p_values:\n",
    "    for d in d_values:\n",
    "        for q in q_values:\n",
    "            order = (p, d, q)\n",
    "            try:\n",
    "                # Perform cross-validation\n",
    "                cv_scores = []\n",
    "                cv_scores2 = []\n",
    "                cv_scores3 = []\n",
    "                for train_index, test_index in tscv.split(filled_data):\n",
    "                    train, validation = filled_data.iloc[train_index], filled_data.iloc[test_index]\n",
    "                    model = ARIMA(train, order=order)\n",
    "                    model_fit = model.fit()\n",
    "                    predictions = model_fit.forecast(steps=len(validation))\n",
    "                    rmse = np.sqrt(mean_squared_error(validation, predictions))\n",
    "                    mae = mean_absolute_error(validation, predictions)\n",
    "                    mse = mean_squared_error(validation, predictions)\n",
    "                    absolute_errors = np.abs(validation - predictions)\n",
    "                    absolute_errors_numeric = pd.to_numeric(absolute_errors, errors='coerce')\n",
    "                    mape = np.mean(np.abs(absolute_errors / validation)) * 100\n",
    "\n",
    "                    mape = np.mean(np.abs((validation - predictions) / validation)) * 100  \n",
    "                    print(f\"Absolute Error Data Type: {absolute_errors.dtypes}\")\n",
    "                    print(\"RMSE\")\n",
    "                    print(rmse)\n",
    "                    print(\"MAE\")\n",
    "                    print(mae)\n",
    "                    print(\"MSE\")\n",
    "                    print(mse)\n",
    "                \n",
    "\n",
    "                    cv_scores.append(rmse)\n",
    "                    cv_scores2.append(mae)\n",
    "                    cv_scores3.append(mse)\n",
    "\n",
    "                    print(f\"Final RMSE: {min(cv_scores)}\")\n",
    "                    print(f\"Final MAE: {min(cv_scores2)}\")\n",
    "                    print(f\"Final MSE: {min(cv_scores3)}\")\n",
    "                    print(f\"Final MAPE: {mape}\")\n",
    "                    print(f\"MAPE data type: {mape.dtypes}\")\n",
    "                    print(f\"Data Type: {filled_data.dtypes}\")\n",
    "                    print(f\"Data Type - Predictions: {predictions.dtypes}\")\n",
    "                    print(f\"Absolute Errors: {absolute_errors_numeric}\")\n",
    "                    print(f\"Filled Data Null Count: {filled_data.isnull().sum()}\")\n",
    "                    print(f\"Predictions Null : {predictions.isnull().sum()}\")\n",
    "                    print(f\"Predictions: {predictions}\")    \n",
    "\n",
    "                mean_cv_score = np.mean(cv_scores)\n",
    "                #if mean_cv_score < best_score:\n",
    "                best_score, best_model = mean_cv_score, model\n",
    "                best_order = order\n",
    "            except Exception as e:\n",
    "                print(f\"Error fitting model: {e}\")\n",
    "                continue  # Handle potential model fitting errors\n",
    "\n",
    "# Use best_model for final forecasting (trained on the entire dataset)\n",
    "best_model = ARIMA(validation, order=order)\n",
    "best_model_fit = best_model.fit()\n",
    "predictions = best_model_fit.forecast(steps = len(validation))\n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(validation, predictions))\n",
    "mae = mean_absolute_error(validation, predictions)\n",
    "mape = np.mean(np.abs((validation - predictions) / validation)) * 100\n",
    "\n",
    "print(f\"Predictions Length: {len(predictions)}\")\n",
    "print(f\"Validation Length: {len(validation)}\")\n",
    "\n",
    "predictions_limited = predictions[:len(validation)]  # Adjust slicing based on prediction size\n",
    "print(f\"Predictions Limited Length: {len(predictions_limited)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d991ee60",
   "metadata": {},
   "source": [
    "Code for the visuals for the final model performance results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e993be",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "plt.scatter(validation, predictions)\n",
    "plt.plot(validation[-20:], validation[-20:], color=\"red\", linestyle=\"--\")\n",
    "plt.xlabel(\"Actual Values\")\n",
    "plt.ylabel(\"Predicted Values\")\n",
    "plt.title(\"Predicted vs. Actual Values\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
